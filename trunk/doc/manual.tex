\documentclass[12pt]{amsart}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{amscd}
\usepackage{epsf}
\usepackage{color}
\usepackage{amsopn,fixltx2e,microtype}
\usepackage{amsfonts}
\usepackage{euscript,mathrsfs} 
\usepackage{graphicx}    % standard LaTeX graphics tool
\usepackage{a4}
\usepackage{bbm}

\newcommand{\set}[1]{\left\lbrace #1 \right\rbrace} % standard set
\newcommand{\defas}{\mathrel{\mathop{:}}=}   % Definition
\renewcommand{\subset}{\subseteq}  % Subsetsymbols
\renewcommand{\supset}{\supseteq}
\newcommand{\todo}[1]{\{ \huge{Todo:}\normalsize #1 \}} % Todo
\DeclareMathOperator*{\bigtimes}{\textnormal{\Large $\times$}} % Cartesian Product
\DeclareMathOperator*{\supp}{supp}
\DeclareMathOperator*{\cl}{cl}
\DeclareMathOperator*{\spann}{span}
\newcommand{\comm}[1]{ }
\newcommand{\comment}[1]{ }
\providecommand{\abs}[1]{\left\lvert#1\right\rvert} 
\providecommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\E}{\mathbb{E}} % expectation
\renewcommand{\P}{\mathbb{P}} % probability
\renewcommand{\d}{\mathrm{d}\:\!} %differential 
\newcommand{\e}{\mathrm{e}} % Eulers Number
\renewcommand{\i}{\mathrm{i}} % Imaginary Unit
\newcommand{\Ind}[1]{\mathbbm{1}_{\lbrace #1 \rbrace}} % indicator function
\newcommand{\Id}{\mathbbm{1}} % The identity map
\newcommand{\ie}{i.e.\;}  % make i.e. not the end of a sentence
\newcommand{\eg}{e.g.\;}  % make e.g. not the end of a sentence
\newcommand{\etc}{etc.\;}
\newcommand{\iid}{i.i.d.\;} % make i.i.d. not the end of sentence
\newcommand{\cipi}{\texttt{CIPI}\,}

\theoremstyle{plain}% default
\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{exmp}[thm]{Example}

\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}
\newtheorem{case}{Case}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title[CIPI]{Computing Information Projections Iteratively with \cipi}

\author{L. Steiner and T. Kahle}
\address{Max Planck Institute for Mathematics in the Sciences, Inselstrasse 22, D-04103 Leipzig, Germany}
\email{\{kahle,steiner\}@mis.mpg.de}

\date{\today}

\comm{
\begin{abstract}
  some abstract\ldots
\end{abstract}
}

%% Maketitle here would insert a pagebreak
\maketitle

\section{Introduction}
In this paper we describe the software tool \cipi. It can be used to compute so
called information projections. Given an exponential family $\mathcal{E}$ of probability
measures associated to a complete hypergraph on $\set{1,\ldots,N}$ (a ``hierarchical model'')
\cipi computes, for a given measure $\hat{P}$, the minimizer of 
\begin{equation*}
  D(\hat{P}\parallel Q) \qquad Q\in\mathcal{E}, 
\end{equation*} where $D$ is the \emph{Kullback-Leibler Divergence}. These
Minimizers are of high intereset in several applications.
The paper is organized as follows. In Section \ref{sec:theory} we introduce the
theoretical background. In Section \ref{sec:usage} the technical details of
using \cipi are described. 

\begin{exmp}
  Consider the problem of contingency tables. Assume, in a survey 1000 people in
  Country A and Country B have been asked bout their smoking behaviour. This
  yielded the following results
  \begin{center}
  \begin{tabular}[]{|l|c|c|}
    \hline
    & male & female \\
    \hline
    smoking & 152 & 271 \\
    non-smoking & 319 & 258\\
    \hline
  \end{tabular}\hspace{1cm}
  \begin{tabular}[]{|l|c|c|}
    \hline
    & male & female \\
    \hline
    smoking & 219 & 331 \\
    non-smoking & 178 & 272\\
    \hline
  \end{tabular}
\end{center}

  A natural question to ask, is for independence of the factors by
  measuring the Kullback-Leibler Divergence of the empirical distribution to the
  ``closest'' approximation within the set of indipendent distributions, \ie the
  set 
  \begin{equation*}
    \mathcal{E}_1 \defas \set{P : P\left( x_1,x_2 \right)) = P_1(x_1)P_2(x_2)}
  \end{equation*}

  We now create two files called \texttt{A.dat}, \texttt{B.dat} 
  containing, one sample per line. So \texttt{A.dat} would contain 152 lines
  with the content ``00''. 271 lines with ``01'', 319 lines with ``10'' and 258
  lines containing ``11''. \cipi will build the emperical measure from the data.
  \texttt{B.dat} is filled correspondingly with the data of the second table. 

  Then we need to specify the parameters in a file called \texttt{PARAM}. We
  modify the example which is included, such that it has the follwing entries.
  \begin{verbatim}
  N=2
  Alphabet=01
  SetIterations=100
  InputType=Sample
  Hypergraph=
  OutputFilePrefix=A
  NumberOfProcesses=3
  \end{verbatim}
  Specifying an empty hypergraph will compute the projections to the exponential
  families of $k$-Interactions. \texttt{cipi} will output several files and the
  following messages.
  \begin{verbatim}
  writing p(0) to file A_p(0).txt
  writing p(1) to file A_p(1).txt
  writing p empirical to file A_p_empirical.txt
  2 : 0.0185144
  1 : 0.0135883

  run cipi in 0s
  read input in 0s
  calculate projection in 0s
  \end{verbatim} We are
  primarily interested in the outputfile \texttt{A\_p(1).txt}. It contains the
  projection of the empirical measure (saved in \texttt{A\_p\_empirical.txt} to
  the exponential family $\mathcal{E}_1$. The KL-distance of the empirical
  measure and this projection equals 0.0185144.
  
  When we run \cipi on \texttt{B.dat} we get the following output
  \begin{verbatim}
  writing p(0) to file A_p(0).txt
  writing p(1) to file A_p(1).txt
  writing p empirical to file A_p_empirical.txt
  2 : 3.56572e-06
  1 : 0.026379

  run cipi in 0s
  read input in 0s
  calculate projection in 0s
  \end{verbatim}

  In this case, the distance of the empirical measure to the familiy of
  independent distributions is very small. This indicates independence of the
  factors. 
\end{exmp}

\section{Theory}
\label{sec:theory}
\subsection{Exponential Families in the Simplex of Probability Measures}
\label{sec:expon-famil-simpl}
For all of the following considerations let $\Omega$ be a non-empty
finite set. The set of all real functions on $\Omega$ is naturally
identified with the real vector space $\mathbb{R}^{\Omega}$. A
probability distribution $P$ on $\Omega$ is a function satisfying
\begin{equation*}
  P(\omega) \geq 0 \quad \forall \omega \in \Omega, \qquad \sum_{\omega\in\Omega} P(\omega) = 1.
\end{equation*}
The set of all such probability distributions therefore has a natural
identification with
\begin{equation*}
  \overline{\mathcal{P}}(\Omega) \defas \set{P = \left(P(\omega)\right)_{\omega\in\Omega} 
    \in \mathbb{R}^{\Omega} : P(\omega) \geq 0 \forall \omega, \sum_{\omega\in\Omega} P(\omega) = 1} 
\end{equation*}
a $\left(\abs{\Omega}-1\right)$-dimensional simplex in
$\mathbb{R}^{\Omega}$. Because of this fact we will often speak of the
\emph{simplex of probability distributions}. \index{simplex of
  probability distributions}

For every distribution $P\in\overline{\mathcal{P}}$ we denote
\begin{equation*}
  \supp P \defas \set{\omega \in \Omega : P(\omega) > 0}
\end{equation*}
its support and furthermore define
\begin{equation*}
  \mathcal{P}(\Omega) \defas \set{P\in\overline{\mathcal{P}}(\Omega) : \supp P = \Omega}
\end{equation*}
the distributions with full support.

\begin{defn} Let $\mathcal{Q} \subset
  \mathbb{R}^{\Omega}$ be a linear subspace of $\mathbb{R}^{\Omega}$.
  We call the set
  \begin{equation*}
    \mathcal{E} = \set { P \in \mathcal{P}(\Omega) : P(\omega)= \frac{\e^{H(\omega)}}
      {\sum_{\omega\in\Omega} \e^{H(\omega)}} , H \in \mathcal{Q}} 
  \end{equation*}
  the \emph{exponential family}\index{exponential family!of a
    subspace} of the subspace $\mathcal{Q}$.
\end{defn}

\subsection{Interaction Spaces and their Exponential Families}

We now look at finite sets of composite structure.  To define them let
$V\defas \set{1,\ldots,N}$ be a finite, non-empty set of so called
\emph{nodes} (or \emph{units}). \index{node} \index{unit}  For every
node $i \in V$ let $\Omega_i$ be a finite, non-empty set. Given a
subset $A\subset V$ we define $\Omega_A \defas \bigtimes_{i\in A}
\Omega_i$ to be called the \emph{configuration space}
\index{configuration space} on $A$.  Furthermore for every subset
$A\subset V$ we have natural projections
\begin{equation*}
  X_A : \Omega_V \to \Omega_A \qquad (\omega_i)_{i\in V} 
  \mapsto (\omega_i)_{i\in A}.
\end{equation*}

One can imagine $\Omega_V$ as a model of a finite world. It has
finitely many (namely $\abs{V}$ = N) locations $i$, each of which can
be in $\abs{\Omega_i}$ states. The number of total
states of the world then is $\abs{\Omega_V} = \prod_{i\in V}
\abs{\Omega_i}$.

This model appears in a large variety of applications, for instance in
the theory of statistical models, where each node carries a random
variable and interactions between the units are studied. In the
present work we will mostly deal with the case of binary units, which
means $\Omega_i = \set{0,1}$ for all $i\in V$. Most of the
constructions extend to the general case in a straightforward manner.

\begin{exmp}[Two binary units]
\label{sec:twobinaryunitsexample}
The simplest example is the case of two binary units. We will now
explicitly give all the terms defined so far for this case. We have
two units $V = \set{1,2}$ each of which can be in two states $\Omega_1
= \Omega_2 = \set{0,1}$. Therefore $\Omega_V = \set{0,1}^2 =
\set{(0,0),(0,1),(1,0),(1,1)}$. Note that here and in the following,
if we need an ordering of the configurations, we will order them like
binary numbers. The associated real vector space has dimension four
and the simplex of probability distributions on $\Omega_V$ is just the
three dimensional tetrahedron.
\end{exmp}

We follow the rather classical idea of \cite{darrochspeed83}, also
used in \cite{lauritzen96,ayknauf06} to decompose the real vector
space $\mathbb{R}^{\Omega_V}$ according to the interaction between the
units. This will lead to an orthogonal decomposition.

\comment{

The full configuration space $\Omega_V$ of our system has a
composite structure. We want to decompose As an example one could ask
for the set of probability measures where never more than two units
interact (\ie there is a statistical dependence between the units). It
will turn out that it is given by an exponential family based on a
certain subspace which we will construct next.

In this work we clarify the role of this subspaces and add useful
tools to the theory by explicitly giving a basis for the case of
binary units.
}

\subsubsection{Interaction Spaces}
\label{sec:orth-decomp-of-ROMEGAV}
Exposing the compositional structure of $\Omega_V$ we can decompose
elements $H \in \mathbb{R}^{\Omega_V}$ into orthogonal components. For
every $A\subset V$, every $\omega \in \Omega_V$ can be decomposed into
$(\omega_A,\omega_{V\backslash A})$, where $\omega_A \defas X_A(\omega)$.  We define the subspace
$\mathcal{Q}_A \subset \mathbb{R}^{\Omega_V}$ of functions not
depending on configurations $\omega_{V\backslash A}$.
\begin{defn} We call 
  \begin{equation*}
  \label{eq:definitionsinteractionsubspace}
  \mathcal{Q}_A \defas \set{H\in \mathbb{R}^{\Omega_V} :
    H(\omega_A,\omega_{V\backslash A}) =  H(\omega_A,\omega'_{V\backslash A}) 
    \,\forall \omega_A \in \Omega_A, \omega_{V\backslash A},\omega'_{V\backslash A} \in \Omega_{V \backslash A} }
\end{equation*} the \emph{interaction space of the set $A$}. \index{interaction space!of a set}
\index{QA@$\mathcal{Q}_A$}
\end{defn}
Note that $\mathcal{Q}_\emptyset$ is the space of constants and
$\mathcal{Q}_V = \mathbb{R}^{\Omega_V}$. The subspace $\mathcal{Q}_A$
is $\vert \Omega_A \vert$ dimensional and by choosing a scalar product
one can find projections onto it. 

\subsubsection{Interaction Spaces of Hypergraphs}
We now want to consider exponential families of interactions spaces.
The most general construction method is to consider a so called
hypergraph.

\begin{defn}
  A \emph{hypergraph}\index{hypergraph} $\mathcal{A}$ is a subset of
  the power set $2^V$.
\end{defn}

\begin{rem}
  For technical reasons we consider only \emph{complete hypergraphs},
  \index{complete hypergraph} \ie $A\in \mathcal{A}, B\subset A
  \Rightarrow B\in\mathcal{A}$. Furthermore we assume that every set
  of order one is contained in $\mathcal{A}$, otherwise the model
  could be reduced by removing the units which are not represented.
  For notational convenience we will often consider a numbering of the
  non-empty elements of $\mathcal{A}$. We will then denote $s \defas
  \abs{\mathcal{A}}-1$ and have $\mathcal{A} = \set{\emptyset,
    A_1,\ldots,A_s}$. \index{hypergraph!numbering of elements}
  \index{Ai@$A_i$}
\end{rem}


\begin{defn}
  For every complete hypergraph $\mathcal{A}$ of subsets of $V$ we
  call the subspace
\begin{equation*}
  \label{eq:definitionofhierarchicalsubspaces}
  \mathcal{Q}_\mathcal{A} \defas \sum_{A\in\mathcal{A}} \mathcal{Q}_A
\end{equation*}  
the \emph{interaction space associated with $\mathcal{A}$}.  \index{interaction space!of a hypergrah}

Furthermore we call the exponential family of
$\mathcal{Q}_\mathcal{A}$
\begin{equation*}
  \mathcal{E}_{\mathcal{A}} \defas
  \set { P \in \mathcal{P} : P(\omega)= \frac{\e^{H(\omega)}}
    {\sum_{\omega\in\Omega} \e^{H(\omega)}} , H \in \mathcal{Q}_\mathcal{A}} 
\end{equation*} 
the \emph{exponential family associated with the complete hypergraph
$\mathcal{A}$}. \index{exponential family!of a hypergraph}
\end{defn}

\begin{rem} In the literature the interaction spaces of hypergraphs
  are sometimes called \emph{hierarchical model subspaces}.
  \index{hierarchical model subspace} They play a key role in the
  analysis of statistical models. 
\end{rem}

\begin{exmp} \label{graphmod} We consider two simple examples of
  hypergraphs and corresponding exponential families. 
  \begin{description}
  \item[Graphical Models] Let $G = (V,E)$ be an undirected graph, and
  define
\begin{equation*}  
  \mathcal{A}_G \defas \{ C \subseteq V : 
  \mbox{$C$ is a clique with respect to $G$}\} \, .
\end{equation*} 
Here, a {\em clique\/} is a set $C$ that satisfies the following
property:
\[
i,j \in C, \;\; i \not= j \quad \Rightarrow \quad \mbox{there is an
  edge between $i$ and $j$} \, .
\] 
The exponential family ${\mathcal E}_{{\mathcal A}_G}$ is
characterized by Markov properties with respect to $G$ (see
\cite{lauritzen96}).
\item[Interaction order] The hypergraph associated with a given
  interaction order $k \in \{0,1,2, \dots,N\}$ is defined as
\begin{equation*}
  {\mathcal A}_k \defas \set{ A \subset V : \abs{A} \leq k} \, .
\end{equation*} 
This gives us a corresponding hierarchy of exponential families
studied in \cite{Amari01, ayknauf06}:
\[ {\mathcal E}_{{\mathcal A}_1} \; \subseteq \; {\mathcal
  E}_{{\mathcal A}_2} \subseteq \; \dots \subseteq \; {\mathcal
  E}_{{\mathcal A}_N} = {\mathcal P}({\Omega}_V)\, .
\] Below will develop this example in the context of statistical
learning theory.
\end{description}
\end{exmp}

\subsection{Basics from Information Theory}

\label{sec:informandstatistics}
In the following we will review basic notions of information theory. A
standard reference is \cite{cover91}. We use $0\log 0 = 0$ and through
the whole text by $\log$ we refer to the natural logarithm.

\subsubsection{Kullback Leibler Distance}
As a central concept we define a notion of distance in
$\tilde{\mathcal{P}}(\Omega_V)$.
\begin{defn}\label{sec:kullb-leibl-dist-definition} Given $P,Q \in
  \tilde{\mathcal{P}}(\Omega_V)$ we call
  \begin{equation*}
  \label{eq:finitekullbackleibler}
  D ( P \parallel Q) =
  \begin{cases}
    \sum_{\omega\in\Omega_V} P(\omega) \log\frac{P(\omega)}{Q(\omega)}
    & \text{if } \supp Q \supset \supp P \\
    \infty & \text{else}
  \end{cases}
\end{equation*}
the \emph{Kullback Leibler distance of the distributions $P$ and $Q$}.
\index{Kullback-Leibler distance} It is also called \emph{relative
  entropy}\index{relative entropy} or \emph{information
  divergence}.\index{information divergence}
\end{defn}
 
\begin{rem}
  This concept was introduced in \cite{kullbackleibler51} by Kullback
  and Leibler.\footnote{For the definition on general measurable
    spaces see for instance \cite{georgii88}.} They also considered a
  symmetric version $D(P\parallel Q) + D(Q\parallel P)$ which they
  called the \emph{divergence}.
\end{rem} 

Although not a metric, $D(P\parallel Q)$ is non-negative and zero if
and only if $P=Q$.  This statement is, in the finite case, a
reformulation of the well known log sum inequality, namely for
non-negative numbers $p_1,\ldots,p_l$ and $q_1,\ldots,q_l$ one has
\begin{equation*}
  \label{eq:logsuminequality}
  \sum_{i=1}^l p_i \log \frac{p_i}{q_i} \geq \left(\sum_{i=1}^l p_i \right) \frac{\sum_{i=1}^l p_i}{\sum_{i=1}^l q_i}. 
\end{equation*}

By convergence of probability distributions $P_n \to P$ in the
following we will mean point wise convergence, \ie convergence in
$\mathbb{R}^{\Omega_V}$. All used topological concepts like
continuity, open sets, \etc are meant for the topology of point wise
convergence.  Note that $D(P\parallel Q)$ is lower semicontinuous in the pair
$(P,Q)$.

\subsubsection{Lumpings}
\label{sec:definitionoflumpings}  
\begin{defn}
  Let $\mathcal{B} = \set{B_1,\ldots,B_k}$ be a partition of
  $\Omega_V$.  For a given distribution $P$ we call the distribution
  $P^{\mathcal{B}} : \set{1,\ldots, k} \to \left[0,1\right]$,
  $P^{\mathcal{B}}(j) = \sum_{\omega\in B_j}P(\omega)$ a
  \emph{$\mathcal{B}$-lumping} \index{Lumping} of P.
\end{defn}
A useful property of lumpings follows from the log sum inequality:
\begin{lem}
For any $\mathcal{B}$-lumpings of $P,Q$
  \begin{equation*}
    \label{eq:divergenceinequalityforlumpings}
    D(P \parallel Q) \geq D(P^{\mathcal{B}}\parallel Q^{\mathcal{B}})
  \end{equation*}
\end{lem}
\begin{proof}
  \begin{align*}
    \label{eq:proofofdivergencestuff127}
    D(P \parallel Q) = & \sum_{\omega\in\Omega_V} P(\omega) \log \frac{P(\omega)}{Q(\omega)} = \sum_{i=1}^k \sum_{\omega \in B_i} P(\omega) \log \frac{P(\omega)}{Q(\omega)} \\
    \geq & \sum_{i=1}^k \left(\sum_{\omega\in B_i} P(\omega)\right) \log \frac{\sum_{\omega\in B_i} P(\omega)}{\sum_{\omega\in B_i}Q(\omega)} \\
    = & \sum_{i=1}^k P^{\mathcal{B}}(i) \log \frac{P^{\mathcal{B}}(i)}{Q^{\mathcal{B}}(i)} = D(P^{\mathcal{B}}\parallel Q^{\mathcal{B}})
  \end{align*}
\end{proof}

\subsection{Information Projections}
In this section we move on to a broader perspective of the set of
probability measures on $\Omega_V$ by incorporating geometrical ideas.
A standard reference is \cite{Amari00}.

\subsubsection{I- and rI-Projections}
In information geometry the information divergence is generally
interpreted as some kind of distance function for probability
measures. Although not symmetric, it shows a lot of properties similar
to the squared Euclidean distance. Of special interest are the
minimizer of the information divergence, giving a notion of
projections. For a sophisticated overview of the topic see
\cite{CsiMa03}. \index{information projection|(}
\begin{defn}
  \label{sec:i-ri-projections-definition}
  An \emph{I-projection(information projection)}\index{I-projection}
  of a distribution $Q$ onto a nonempty, closed, convex set $\Pi$ of
  probability distributions on $\Omega_V$ is a distribution $P^*$ such
  that
  \begin{equation*}
    \label{eq:definitionofrIprojection}
      D(P^*\parallel Q) = \min_{P\in\Pi}D(P\parallel Q).
  \end{equation*}
  By a \emph{log-convex}\index{log-convex} set of probability measures
  we mean a set that contains all log-convex combinations of not
  mutually singular pairs of probability measures in the set. An
  \emph{rI-projection(reverse information projection)}
  \index{rI-projection} of a distribution $Q$ onto a nonempty
  log-convex set $\Pi$ of probability distributions on $\Omega_V$ is a
  $P^*$ such that
  \begin{equation*}
    \label{eq:definitionofIprojection}
      D(Q\parallel P^*) = \min_{P\in\Pi}D(Q\parallel P).
  \end{equation*}
\end{defn}
\index{information projection|)}

\begin{rem}[Linear and exponential families]
  Linear families are convex and always closed. Exponential families
  are log convex, and need not to be closed. We denote the closure of
  an exponential family $\mathcal{E}$ by $\cl\mathcal{E}$.
\end{rem}

In the following we suppose $\supp Q = \Omega_V$. $D(P\parallel Q)$
then is a strictly convex, continuous function in $P$ and therefore
the I-projection $P^*$ onto a convex set $\Pi$ exists and is
unique.\footnote{This restriction does not touch our applications
  since we only I-project elements of exponential families, having
  full support.}  In a convex set $\Pi$ there exists a distribution
whose support contains all the supports of the distributions in $\Pi$.
This set will be defined as $\supp \Pi$, the support of $\Pi$.

We will now collect a number of facts about the I-projection and
information divergence without giving proofs. These and more detailed
descriptions can be found in \cite{csiszarshields04}.

\begin{thm}
  \label{sec:pythagoreaninequalitytheorem}
  Let $\Pi$ be a convex set of probability measures. Let $P^*$ be the
  I-projection of a measure Q to $\Pi$. Then $\supp P^* = \supp \Pi$
  and
  \begin{equation}
    \label{eq:divergenceinequality1}
    D(P\parallel Q) \geq D(P\parallel P^*) + D(P^*\parallel Q) \quad \forall P \in \Pi.
  \end{equation}
  Furthermore if the inequality holds for some $P^*\in\Pi$ and all
  $P\in\Pi$ then already $P^*$ must be the I-projection of $Q$ onto
  $\Pi$.
\end{thm}

Next lets examine the situation where we have equality in
(\ref{eq:divergenceinequality1}). Assume we have are given an
exponential family $\mathcal{E}$ and linear family $\mathcal{L}$,
defined by the same set of functions and reference measure. Then we
have the following

\begin{thm}[Pythagorean Theorem]
\label{sec:pythagoreantheorem}
The I-projection $P^*$ of Q onto a linear family $\mathcal{L}$
satisfies
  \begin{equation*}
    \label{eq:pythagoreanidentity}
    D(P\parallel Q) = D(P\parallel P^*) + D(P^*\parallel Q), \, \forall P \in \mathcal{L}.
  \end{equation*}
  Furthermore, if $\supp (\mathcal{L})=\Omega_V$ then $\mathcal{L}\cap
  \mathcal{E} = \set{P^*}$, and in general $\mathcal{L}\cap \cl
  \mathcal{E} = \set{P^*}$.
\end{thm}

\begin{cor}
  For a linear family $\mathcal{L}$ and an exponential family
  $\mathcal{E}$ defined by the same functions $f_1,\ldots,f_k$, the
  intersection $\mathcal{L}\cap\cl\mathcal{E}$ consists of the single
  distribution $P^*$ and
  \begin{equation*}
    \label{eq:pythagoreanidentity2}
    D(P\parallel Q) = D(P\parallel P^*) + 
    D(P^*\parallel Q),\, \forall P \in \mathcal{L} \quad \forall Q\in\cl\mathcal{E}.
  \end{equation*}
\end{cor}
From this, in the case of exponential and linear families the dual
picture of I-projections and rI-projections becomes clear. The unique
measure $P^*$ of the theorem is at the same time the I-projection of
every measure in the exponential family $\mathcal{E}$ to the linear
family $\mathcal{L}$, and also the rI-projection of every measure in
$\mathcal{L}$ to $\mathcal{E}$. Now a geometrical picture emerges that
makes the similarity between the information divergence and the
squared euclidean distance clear (see Figure
\ref{fig:pythagoreanfigure}).
\begin{figure}[htbp]
  \centering
  \input{stupidpic.pstex_t} 
  \caption{Geometrical Picture of Linear and Exponential families. The
    intersection of linear and exponential families contain only a
    single distribution. It can be seen that $\mathcal{L}$ and
    $\mathcal{E}$ intersect orthogonally in a certain sense and like
    the squared euclidean distance $D$ fulfills a Pythagorean
    identity. }
  \label{fig:pythagoreanfigure}
\end{figure}

\subsection{Projections and Maximum Likelihood}
In this section we will employ methods from information geometry to
see that in our case we can find the maximum likelihood estimates as
Information-projections. Our main tool will be the theory of
exponential families and linear families. Recall that the sets of
distributions we are using are exponential families spanned by the
interaction spaces. \cite{csiszarshields04} gives a good overview of
the methods used in information geometric theory of maximum likelihood
estimation.

\begin{thm}
\label{sec:MLE-Lemma}
A Maximum Likelihood Estimator in a set of feasible distributions is
the same as a minimizer of $D(\hat{P}\parallel P)$ for $P$ in that
set, $\hat{P}$ being the empirical distribution of the sample.
\end{thm}
\begin{proof}
  The theory of maximum likelihood tells us to maximize the likelihood
  functional
  \begin{equation*}
    L(P) \defas \sum_{\omega \in \Omega_V} \log P(\omega) \hat{P}(\omega).
  \end{equation*}
  By adding the constant $- \sum_{\omega\in\Omega_V} \hat{P} \log
  \hat{P}$, which does not change the maximizer, we can see that the
  method of maximum likelihood is equivalent to finding the minimizer
  of $D(\hat{P}\parallel P)$.
\end{proof}

We continue to formulate the equivalence of Maximum Likelihood
estimates and Information-projections in our setting of exponential
and linear families modeled on interaction spaces.
\begin{thm}
\label{sec:MLEequalsprojectiontheorem}
Let the set of feasible distributions for a maximum likelihood
estimation be the exponential family
\begin{equation}
  \label{eq:expfamilyformle}
  \mathcal{E}_\mathcal{A} = \set{ P : P(\omega) = 
    \frac{1}{Z} \exp 
    \left(\sum_{i=1}^{s} c_i e_{A_i}(\omega)\right), 
    c \in \mathbb{R}^s },
\end{equation}
Then given a sample $\omega_1,\ldots,\omega_l$, the MLE is unique and
equals the I-projection $P^*$ of the uniform distribution $Q(\omega)
\defas \frac{1}{\abs{\Omega_V}}$ onto the linear family
\begin{equation}
  \label{eq:linfamilymlethm}
  \mathcal{L}_\mathcal{A} = \set{P: \sum_{\omega\in\Omega_V} P(\omega) e_{A_i}(\omega) = \frac{1}{n} \sum_{j=1}^{l} e_{A_i}(\omega_j), 1\leq i \leq s },
\end{equation}
provided that $\supp(\mathcal{L}) = \Omega_V$. If
$\supp(\mathcal{L})\neq \Omega_V$, the MLE does not exist, but $P^*$
will be the MLE in the case if $\cl\mathcal{E}$ rather than
$\mathcal{E}$ is taken as the set of feasible distributions.
\end{thm}
\begin{proof} For the sake of completeness we repeat the argumentation
  of \cite{csiszarshields04}. By definition we have that $\hat{P} \in
  \mathcal{L}$ and hence by the Pythagorean Theorem (section
  \ref{sec:pythagoreantheorem})
  \begin{equation}
    \label{eq:inproofoflinfamthm1}
    D(\hat{P}\parallel P) = D(\hat{P}\parallel P^{*}) + D(P^{*}\parallel P), \, \forall P\in \cl\mathcal{E}
 \end{equation}
 Also we have that $P^* \in \mathcal{E}$ respectively its closure
 depending on whether $\supp(\mathcal{L})=\Omega_V$. Therefore the
 minimum in (\ref{eq:inproofoflinfamthm1}) is uniquely attained for
 $P=P^*$ if $\supp(\mathcal{L})=\Omega_V$ and not attained if
 $\supp(\mathcal{L})\neq\Omega_V$. On the other hand $P^*$ is always the
 unique minimizer of $D(\hat{P}\parallel P)$ subject to
 $P\in\cl\mathcal{E}$. Now the application of Theorem
 \ref{sec:MLE-Lemma} completes the proof.
\end{proof}

With this theorem we have a new notion of projecting to exponential
families as well.

\begin{defn}
  Let $P\in\mathcal{P}(\Omega_V)$. We define $\Pi_{\mathcal{E}} (P)$
  the \emph{projection of $P$ to an exponential family $\mathcal{E}$}
  \index{projection to exponential family} to be the unique element in
  the intersection of the exponential family $\mathcal{E}$ with the
  linear family $\mathcal{L}$ generated by the expectation values of
  $P$. On account of Theorem \ref{sec:pythagoreantheorem}
  $\Pi_{\mathcal{E}}$ equals the rI-projection of $P$ to
  $\mathcal{E}$, since it is the minimizer of $D(P\parallel Q)$ with
  $Q\in\mathcal{E}$.
\end{defn}


\subsection{Computing Projections to Linear Families}
\label{sec:proj-line-famil}
Ultimately we want to explicitly compute projections of empirical
measures to certain exponential families. On the account of the
considerations so far we can equivalently project the uniform
distribution which is contained in the exponential family to the
linear family defined by the expectations of the empirical measure.
Projections to linear families have nice properties and can, in our
case, be computed explicitly.

\subsubsection{Iterative Scaling}
Assume we are given a partition of the configuration space $\Omega_V$,
\ie $\mathcal{B} = (B_1,\ldots,B_k)$ such that $\cup_{i=1}^k B_i =
\Omega_V$ with the $B_i$ being pairwise disjoint. Assume further that
a linear family $\mathcal{L}$ is determined by the marginals on
$\mathcal{B}$, which means we are given numbers
$(\alpha_1,\ldots,\alpha_k)$ such that $\sum_{\omega \in B_i}
P(\omega) = \alpha_i$ (Speaking in the language of Section
\ref{sec:definitionoflumpings} $\mathcal{L}$ is determined by a
$\mathcal{B}$-lumping).

\begin{lem}
\label{sec:simplescalinglemma}
The I-projection of a distribution $Q$ to the linear family
$\mathcal{L}$ is given by
  \begin{equation}
    \label{eq:projectiontosimplefamily}
    P^*(\omega) = c_i Q(\omega) ,\; \omega\in B_i, \quad 
    \mbox{where } c_i = \frac{\alpha_i}{Q(B_i)}.
  \end{equation}
\end{lem}
\begin{proof}
  We have $D(P\parallel Q) \geq D(P^{\mathcal{B}}\parallel
  Q^{\mathcal{B}}) = \sum_i \alpha_i \log \left(\alpha_i\slash
    Q(B_i)\right) \, \forall P \in \mathcal{L}$. Equality holds for
  the $P^*$ defined in (\ref{eq:projectiontosimplefamily}). Therefore
  with Theorem \ref{sec:pythagoreaninequalitytheorem} $P^*$ must be
  the I-projection of $Q$ onto $\mathcal{L}$.
\end{proof}

Now suppose we have a collection of linear families
$\mathcal{L}_1,\ldots,\mathcal{L}_m $ onto which we can project, for
instance by applying the simple scaling from above. Generate a
sequence of distributions $P_n$ as follows: Set $P_0 = Q$ and let
$P_n$ be the I-projection of $P_{n-1}$ to $\mathcal{L}_i$ where $i
\equiv n\, \pmod m$. This means the projection is cyclically repeated
if $n$ exceeds $m$.

\begin{thm}
\label{sec:iterartiveprojectiontheorem}
  If $\mathcal{L} = \cap_{i=1}^m \mathcal{L}_i \neq \emptyset$ then $P_n \to P^*$ the I-projection of Q onto $\mathcal{L}$. 
\end{thm}
\begin{proof} For the convenience of the reader we give a proof. It is
  taken from \cite{csiszarshields04}.

  Applying the Pythagorean Theorem \ref{sec:pythagoreantheorem} we see
  that for every $P \in \mathcal{L}$ we have that
\begin{equation*}
  \label{eq:inproofofiterativeprojectiontheorem1}
  D(P\parallel P_{n-1}) = D(P\parallel P_n) + D(P_n\parallel P_{n-1}), \, \forall n\in\mathbb{N}.
\end{equation*}
We add the equations for $n\leq N$ to find that 
\begin{equation}
  \label{eq:inproofofiterativeprojectiontheorem2}
  D(P\parallel Q) = D(P\parallel P_0) = D(P\parallel P_N) + \sum_{n=1}^N D(P_n\parallel P_{n-1}).
\end{equation}
By compactness there exists a subsequence $P_{N_k} \to P'$ and then from (\ref{eq:inproofofiterativeprojectiontheorem2}) we have for $N_k \to \infty$
\begin{equation}
  \label{eq:inproofofiterativeprojectiontheorem3}
  D(P\parallel Q) = D(P\parallel P') + \sum_{n=1}^\infty D(P_n\parallel P_{n-1}).
\end{equation}
We will now prove that $P'$ equals the I-projection of $Q$ onto
$\mathcal{L}$ by showing that it fulfills the equality in Theorem
\ref{sec:pythagoreaninequalitytheorem}.  Since the series in
(\ref{eq:inproofofiterativeprojectiontheorem3}) is convergent it is a
Cauchy sequence too and we have $|P_n - P_{n-1}| =
\sum_{\omega\in\Omega_V}|P_n(\omega)-P_{n-1}(\omega)| \to 0$ as
$n\to\infty$. Since $P_{N_k} \to P'$ we have that 
\begin{equation*}
  \label{eq:inproofofiterativeprojectiontheorem4}
  P_{N_k+1} \to P', \quad P_{N_k+2} \to P',\quad  \ldots \quad  P_{N_k+m} \to P'.
\end{equation*}
By construction of the $P_n$ we have that among the elements
$P_{N_k+j}, \, j=1,\ldots,m$ there is one in each $\mathcal{L}_j,\,
j=1,\ldots,m$. It follows that $P'\in\cap\mathcal{L}_i = \mathcal{L}$.
Since $P'\in\mathcal{L}$
(\ref{eq:inproofofiterativeprojectiontheorem3}) holds for it and we
find
\begin{equation*}
  \label{eq:inproofofiterativeprojectiontheorem5}
  D(P'\parallel Q) = \sum_{i=1}^\infty D(P_n\parallel P_{n-1}).
\end{equation*}
Substituting this into (\ref{eq:inproofofiterativeprojectiontheorem3})
again yields
\begin{equation*}
  \label{eq:inproofofiterativeprojectiontheorem6}
  D(P\parallel Q) = D(P\parallel P') + D(P'\parallel Q),
\end{equation*}
proving that $P'$ is the I-projection $P^*$ of $Q$ onto $\mathcal{L}$.
Finally we need to show that $P_n \to P^*$ but this is clear since
$(P_n)_{n\in\mathbb{N}}$ is compact and the subsequence was arbitrary.
Therefore every convergent subsequence of $P_n$ has the same limit
$P^*$ and $P_n\to P^*$.
\end{proof}

\subsection{Decompositions of Linear Families}
We now move back to our special setting and consider linear families
given by marginals on sets $A\subset V$. To do so assume again that a
sample $\omega_1,\ldots,\omega_l$ of the unknown distribution is
given. \comment{ To project to these families we can apply the simple
  scaling argument given above. Then we will show that this is
  applicable for linear families $\mathcal{L}_\mathcal{A}$ constructed
  from hypergraphs. }

\begin{defn}
  To every $A\subset V$ define
\begin{equation*}
  \label{eq:definitionsofLa}
  \mathcal{L}_A \defas \set {P : \sum_{\omega} P(\omega) \Ind{X_A = \omega_A} = \alpha_{A,\omega_A} , \omega_A \in \Omega_A}
\end{equation*} 
where $\alpha_{A,\omega_A}$ is the relative frequency of the
occurrence of the event specified by the indicator $\Ind{X_A =
  \omega_A}$, \ie the relative frequency of the event ``$\omega$ looks
on $A$ like $\omega_A$''. More formally:
\begin{equation*}
  \alpha_{A,\omega_A} \defas \frac{1}{l} 
\abs{\set{ \omega_i : X_A(\omega_i) = \omega_A, \, i=1,\ldots,l }}
\end{equation*}
$\mathcal{L}_A$ will be called the \emph{linear family generated by
  the marginals on A} or just \emph{marginal family}.\index{linear
  family!generated by marginals} \index{marginal family}
\end{defn}

In other words, a marginal family is defined as the set of probability
measures for which all the expected values of occurrence of $\omega_A$
on $A$ exactly equal the mean values over the sample.

We note that the cylinder sets $\set{X_A = \omega_A} ,
\omega_A\in\Omega_A$ define a partition of the configuration space
$\Omega_V$. For fixed $A$ they are all disjoint. Therefore to project
on $\mathcal{L}_A$ for fixed $A\subset V$ we can apply the simple
scaling of Lemma \ref{sec:simplescalinglemma}.

\comment{
We will now write $\mathcal{L}_k$ as an intersection of these very
handy simple to compute families and can then apply Theorem
\ref{sec:iterartiveprojectiontheorem} to find the projection onto
$\mathcal{L}_k$ by iteratively projecting onto specific
$\mathcal{L}_A$'s, \ie by iterated rescaling.

\begin{thm}
\label{sec:decompositionoflktheorem}
  \begin{equation*}
    \label{eq:inclusionsoflinearfamilies}
     \mathcal{L}_k = \bigcap_{\substack{A\subset V \\ |A| \leq k}}\mathcal{L}_A = \bigcap_{\substack{A \subset V \\ |A| = k}} \mathcal{L}_A
  \end{equation*}
\end{thm}
\begin{proof}
  We start with the equality 
  \begin{equation}
    \label{eq:toprovefirstinlineartheorem}
    \mathcal{L}_k = \bigcap_{\substack{A \subset V \\|A| = k}} \mathcal{L}_A.
  \end{equation} To see it we observe that in the definition
  of the linear families we take all $P\in \mathbb{R}^{\Omega_V}$ that
  fulfill certain linear conditions. We have proved equality if we
  show that these linear constraints are the same on both sides of the
  equation. The space of constraints for $\mathcal{L}_k$ is spanned by
  the following set $\set{e_A : A\subset V, |A| \leq k}$ . The
  constraints of the intersection on the right hand side of
  (\ref{eq:toprovefirstinlineartheorem}) are spanned by
  $\set{\Ind{X_A=\omega_A} : \omega_A\in \Omega_A, A\subset V, |A| =
    k}$. Therefore the theorem follows if we show that
  \begin{equation}
    \label{eq:spanrequirement} 
  \spann \set{\Ind{X_A=\omega_A} : \omega_A\in \Omega_A,
    A\subset V, |A| = k}  = \spann \set{e_A :
    A\subset V, |A| \leq k}. 
  \end{equation}
  We go back to the construction of the pure interaction spaces in
  section \ref{sec:orth-decomp-of-ROMEGAV} to see that the space
  $\mathcal{Q}_A$ is the same as $\spann\set{\Ind{X_A=\omega_A} :
    \omega_A\in \Omega_A}$ and $\tilde{\mathcal{Q}}_A$ is the same as
  $\spann\set{e_A}$. Because of
  (\ref{eq:reformulationofpureinteractions}) we have that
  \begin{equation*}
 %   \label{eq:inproofsomething0}
    \mathcal{Q}_A = \bigoplus_{B\subset A}\tilde{\mathcal{Q}}_{B}
  \end{equation*}
and therefore
  \begin{equation*}
%    \label{eq:inproofsomething}
      \spann_{\substack{A \subset V \\ |A| = k}} \mathcal{Q}_A = \spann_{\substack{A \subset V \\ |A| = k}} \bigoplus_{B\subset A} \tilde{\mathcal{Q}}_B.
  \end{equation*} From this (\ref{eq:spanrequirement}) follows. 
The equality
 \begin{equation*}
%    \label{eq:toprovesecondinlineartheorem}
      \bigcap_{\substack{A \subset V \\|A| \leq k}} \mathcal{L}_A  = \bigcap_{\substack{A \subset V \\|A| = k}} \mathcal{L}_A.
  \end{equation*} is clear from the fact that for $A \subset B$ follows that $\mathcal{L}_B\subset \mathcal{L}_A$.
\end{proof}

The above proof shows the direction to a very useful generalization of
the theorem. In applications one would like to consider for example
not the set $\mathcal{A} = \set{A\subset V: |A|\leq k}$ as the set on
which we fix the marginals but a different one which encodes
properties of the model one works with. For instance if one has a
priori knowledge that the Hamiltonian only contains local interactions
one would maybe like to restrict the learning process to a family of
the form $\mathcal{A}' = \set{A\subset V : |A|\leq k,\, \forall i,j\in
  A: i\sim j}$. Here $i\sim j$ means that $i$ is a not further
specified neighbor of $j$. We will find the property that $\mathcal{A}$ needs to fulfill to have an equality similar to (\ref{eq:spanrequirement}).
}

\begin{defn}
  Let $\mathcal{A}$ be a complete hypergraph.  The elements $A$ of
  $\mathcal{A}$ for which
\begin{equation*}
  \label{eq:maximalelementsdefinition}
  B \supset A \Rightarrow B = A \qquad \forall B\in\mathcal{A}
\end{equation*}
will be called \emph{maximal elements}\index{maximal element} of
$\mathcal{A}$. The set of maximal elements will be denoted
$\mathcal{A}^m$. \index{Am@$\mathcal{A}^m$}
\end{defn}

Complete hypergraphs have the property that if we construct the
marginal families $\mathcal{L}_A$ for all their maximal elements and
intersect them, we end up with the linear family generated by the
sample means of the $e_A$ where $A$ are all the elements of
$\mathcal{A}$. Recall that $\mathcal{L}_{\mathcal{A}} =
\set{P\in\mathcal{P} : \E_{P}(e_A) = \E_{\hat{P}}(e_A) :
  A\in\mathcal{A}}$, then we have

\begin{thm}
  Let $\mathcal{A}$ be a complete hypergraph, and denote by
  $\mathcal{A}^m$ the maximal elements of $\mathcal{A}$. Then
  \begin{equation*}
    \label{eq:generaldecompositionoflinearfam}
    \mathcal{L}_\mathcal{A} = \bigcap_{A\in\mathcal{A}^m} \mathcal{L}_A.
  \end{equation*}
\end{thm}
\begin{proof} We need to establish the equality
  \begin{equation}
    \label{eq:spanrequirementgeneraldecomptheorem}
    \mathcal{I}_{\mathcal{A}} = 
    \spann\set{{\Ind{X_A=\omega_A}: \omega_A \in \Omega_A, A\in\mathcal{A}^m}},
  \end{equation}
  because this are the linear spaces on which we have constraints and
  same constraint spaces define same linear families. In fact
  \eqref{eq:spanrequirementgeneraldecomptheorem} follows from the easy
  observation that $A\subset B \Rightarrow \mathcal{Q}_A \subset
  \mathcal{Q}_B$ and therefore
  \begin{align*}
    \label{eq:reintegrationofinteractionspaces}
    \spann\set{{e_A : A \in \mathcal{A} }} & = \mathcal{Q}_\mathcal{A} \\
    & = \bigoplus_{A \in \mathcal{A}}\tilde{\mathcal{Q}}_A \\
    & = \sum_{A\in\mathcal{A}} \mathcal{Q}_A \\
    & = \sum_{A\in\mathcal{A}^m} \mathcal{Q}_A.
  \end{align*}
  The last equality is due to the fact that $\mathcal{A}$ is assumed
  to be complete. 
\end{proof}

Summarizing this section we note that we can find the projection of
the empirical distribution $\hat{P}$ to an exponential family by
iteratively projecting the uniform distribution to the families
$\mathcal{L}_A$ where $A\in\mathcal{A}^m$. Each of the projections can
be computed by applying the simple scaling of Lemma
\ref{sec:simplescalinglemma}.

\section{The iterative projection algorithm}
\label{sec:iterative projection algorithm}
\begin{description}
\item[input] $P^{\text{emp}} \in \overline{\mathcal{P}(\Omega_V)}$ (or
  equivalently a sample $\set{\omega_i : i = 1,\ldots,l}$) and the hypergraph
  $\mathcal{A}$. 
  order $k$.
\item[output] an approximation of the minimizer of
  $D(P^{\text{emp}}\parallel Q)$ subject to $Q\in \mathcal{E}_\mathcal{A}$,
  where $\mathcal{E}_\mathcal{A}$ is the exponential family associated to the
  hypergraph $\mathcal{A}$.
\end{description}

\begin{itemize}
\item From $P^{\text{emp}}$ (or the sample) compute the marginals
  $\alpha_{A,\omega_A}$ for all sets $A$ which are maximal in $\mathcal{A}$ and 
  $\omega_A \in \Omega_A$.
  \begin{equation*}
    \alpha_{A,\omega_A} \defas \abs{\set{\omega_i : X_A(\omega_i)
        = \omega_A, i=1,\ldots,l}} = \sum_{\omega : X_A(\omega) = \omega_A}P^{\text{emp}}(\omega).
  \end{equation*}
\item Initialize $P$ with uniform distribution.
\item Run ``sufficiently many times'' through all sets $A$ of order
  $k$ and in every step
  \begin{itemize}
  \item Rescale $P$ according to \footnote{We have defined
      $P(X_A=\omega_A) \defas \sum_{\omega: X_A(\omega) = \omega_A}
      P(\omega)$}
    \begin{equation*}
      P^{\text{new}}(\omega) \defas c_{A,\omega_A}(\omega) P(\omega) \text{ where } c_{A,\omega_A} \defas \frac{\alpha_{A,\omega_A}}{P(X_A = \omega_A)}.
    \end{equation*}
  \end{itemize}
\end{itemize}


\section{Usage}
\label{sec:usage}
\subsection{Parameters}
Every call of \cipi is must be followed by the name of a file containing
parameters. This file allows the following options
\begin{description}
  \item[N] The number of nodes.
  \item[SetIterations] The number of times every set of cardinality $k$ is
    visited in the projection loop.
  \item[InputType] Specifies how the data is read. Possible values are CharacterSequence(datapoint is a long sequence of symbols and the
    statistic is generated using a sliding window of lenght N), Sample(every line is a sample), Integer(every line is a sample and these samples are coded as integer) and Empirical(format of output files from \cipi). 
  \item[Alphabet] specifies the alphabet which is used, \eg ``01'' or
    ``AGCT''. 
  \item[Hpergraph] Describes the Hypergraph of the hierarchical model
  \item[OutputFilePrefix] Beginning of the file name used for the output files. 
  \item[NumberOfProcesses] Maximum number of processes running at the same time. 
\end{description}

\subsection{Output}
Currently \cipi prints to sdtout and creates one file for each hypergraph that
was considered. This file contains the projected measure to that hypergraph.

\bibliographystyle{amsalpha}
\bibliography{alotofstuff}

\end{document}





